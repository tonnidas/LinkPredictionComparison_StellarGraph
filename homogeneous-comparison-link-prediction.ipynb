{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of link prediction with random walks based node embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden",
    "tags": [
     "CloudRunner"
    ]
   },
   "source": [
    "<table><tr><td>Run the latest release of this notebook:</td><td><a href=\"https://mybinder.org/v2/gh/stellargraph/stellargraph/master?urlpath=lab/tree/demos/link-prediction/homogeneous-comparison-link-prediction.ipynb\" alt=\"Open In Binder\" target=\"_parent\"><img src=\"https://mybinder.org/badge_logo.svg\"/></a></td><td><a href=\"https://colab.research.google.com/github/stellargraph/stellargraph/blob/master/demos/link-prediction/homogeneous-comparison-link-prediction.ipynb\" alt=\"Open In Colab\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo notebook compares the link prediction performance of the embeddings learned by Node2Vec [1], Attri2Vec [2], GraphSAGE [3] and GCN [4] on the Cora dataset, under the same edge train-test-split setting. Node2Vec and Attri2Vec are learned by capturing the random walk context node similarity. GraphSAGE and GCN are learned in an unsupervised way by making nodes co-occurring in short random walks represented closely in the embedding space.\n",
    "\n",
    "We're going to tackle link prediction as a supervised learning problem on top of node representations/embeddings. After obtaining embeddings, a binary classifier can be used to predict a link, or not, between any two nodes in the graph. Various hyperparameters could be relevant in obtaining the best link classifier - this demo demonstrates incorporating model selection into the pipeline for choosing the best binary operator to apply on a pair of node embeddings.\n",
    "\n",
    "There are four steps:\n",
    "\n",
    "1. Obtain embeddings for each node\n",
    "2. For each set of hyperparameters, train a classifier\n",
    "3. Select the classifier that performs the best\n",
    "4. Evaluate the selected classifier on unseen data to validate its ability to generalise\n",
    "\n",
    "\n",
    "**References:** \n",
    "\n",
    "[1] Node2Vec: Scalable Feature Learning for Networks. A. Grover, J. Leskovec. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016.\n",
    "\n",
    "[2] Attributed Network Embedding via Subspace Discovery. D. Zhang, Y. Jie, X. Zhu and C. Zhang. Data Mining and Knowledge Discovery, 2019. \n",
    "\n",
    "[3] Inductive Representation Learning on Large Graphs. W.L. Hamilton, R. Ying, and J. Leskovec. Neural Information Processing Systems (NIPS), 2017.\n",
    "\n",
    "[4] Graph Convolutional Networks (GCN): Semi-Supervised Classification with Graph Convolutional Networks. Thomas N. Kipf, Max Welling. International Conference on Learning Representations (ICLR), 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbsphinx": "hidden",
    "tags": [
     "CloudRunner"
    ]
   },
   "outputs": [],
   "source": [
    "# install StellarGraph if running on Google Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  %pip install -q stellargraph[demos]==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbsphinx": "hidden",
    "tags": [
     "VersionCheck"
    ]
   },
   "outputs": [],
   "source": [
    "# verify that we're using the correct version of StellarGraph for this notebook\n",
    "import stellargraph as sg\n",
    "\n",
    "try:\n",
    "    sg.utils.validate_notebook_version(\"1.2.1\")\n",
    "except AttributeError:\n",
    "    raise ValueError(\n",
    "        f\"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>.\"\n",
    "    ) from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stellargraph import StellarGraph, datasets\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "The Cora dataset is a homogeneous network where all nodes are papers and edges between nodes are citation links, e.g. paper A cites paper B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "DataLoadingLinks"
    ]
   },
   "source": [
    "(See [the \"Loading from Pandas\" demo](../basics/loading-pandas.ipynb) for details on how data can be loaded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "DataLoading"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.Cora()\n",
    "display(HTML(dataset.description))\n",
    "# graph, _ = dataset.load(largest_connected_component_only=True, str_node_ids=True)\n",
    "graph, _ = dataset.load(largest_connected_component_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 2485, Edges: 5209\n",
      "\n",
      " Node types:\n",
      "  paper: [2485]\n",
      "    Features: float32 vector, length 1433\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [5209]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "print(graph.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct splits of the input data\n",
    "\n",
    "We have to carefully split the data to avoid data leakage and evaluate the algorithms correctly:\n",
    "\n",
    "* For computing node embeddings, a **Train Graph** (`graph_train`)\n",
    "* For training classifiers, a classifier **Training Set** (`examples_train`) of positive and negative edges that weren't used for computing node embeddings\n",
    "* For choosing the best classifier, a **Model Selection Test Set** (`examples_model_selection`) of positive and negative edges that weren't used for computing node embeddings or training the classifier \n",
    "* For the final evaluation, with the learned node embeddings from the **Train Graph** (`graph_train`), the chosen best classifier is applied to a **Test Set** (`examples_test`) of positive and negative edges not used for neither computing the node embeddings or for classifier training or model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test Graph\n",
    "\n",
    "We begin with the full graph and use the `EdgeSplitter` class to produce:\n",
    "\n",
    "* Test Graph\n",
    "* Test set of positive/negative link examples\n",
    "\n",
    "The Test Graph is the reduced graph we obtain from removing the test set of links from the full graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Sampled 520 positive and 520 negative edges. **\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 2485, Edges: 4689\n",
      "\n",
      " Node types:\n",
      "  paper: [2485]\n",
      "    Features: float32 vector, length 1433\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [4689]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "# Define an edge splitter on the original graph:\n",
    "edge_splitter_test = EdgeSplitter(graph)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from graph, and obtain the\n",
    "# reduced graph graph_test with the sampled links removed:\n",
    "graph_test, examples_test, labels_test = edge_splitter_test.train_test_split(\n",
    "    p=0.1, method=\"global\"\n",
    ")\n",
    "\n",
    "print(graph_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Graph\n",
    "\n",
    "This time, we use the `EdgeSplitter` on the Test Graph, and perform a train/test split on the examples to produce:\n",
    "\n",
    "* Train Graph\n",
    "* Training set of link examples\n",
    "* Set of link examples for model selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Sampled 468 positive and 468 negative edges. **\n",
      "(936, 2) [117328   1237]\n",
      "(702, 2)\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 2485, Edges: 4221\n",
      "\n",
      " Node types:\n",
      "  paper: [2485]\n",
      "    Features: float32 vector, length 1433\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [4221]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "# Do the same process to compute a training subset from within the test graph\n",
    "edge_splitter_train = EdgeSplitter(graph_test)\n",
    "\n",
    "graph_train, examples, labels = edge_splitter_train.train_test_split(p=0.1, method=\"global\")\n",
    "\n",
    "print(examples.shape, examples[len(examples)-1]) # (936, 2)\n",
    "\n",
    "examples_train, examples_model_selection, labels_train, labels_model_selection = train_test_split(examples, labels, train_size=0.75, test_size=0.25, random_state=63)\n",
    "\n",
    "print(examples_train.shape) # (702, 2)\n",
    "\n",
    "print(graph_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a summary of the different splits that have been created in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Examples</th>\n",
       "      <th>Hidden from</th>\n",
       "      <th>Picked from</th>\n",
       "      <th>Use</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Set</th>\n",
       "      <td>702</td>\n",
       "      <td>Train Graph</td>\n",
       "      <td>Test Graph</td>\n",
       "      <td>Train the Link Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Selection</th>\n",
       "      <td>234</td>\n",
       "      <td>Train Graph</td>\n",
       "      <td>Test Graph</td>\n",
       "      <td>Select the best Link Classifier model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test set</th>\n",
       "      <td>1040</td>\n",
       "      <td>Test Graph</td>\n",
       "      <td>Full Graph</td>\n",
       "      <td>Evaluate the best Link Classifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Number of Examples  Hidden from Picked from  \\\n",
       "Split                                                          \n",
       "Training Set                    702  Train Graph  Test Graph   \n",
       "Model Selection                 234  Train Graph  Test Graph   \n",
       "Test set                       1040   Test Graph  Full Graph   \n",
       "\n",
       "                                                   Use  \n",
       "Split                                                   \n",
       "Training Set                 Train the Link Classifier  \n",
       "Model Selection  Select the best Link Classifier model  \n",
       "Test set             Evaluate the best Link Classifier  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        (\n",
    "            \"Training Set\",\n",
    "            len(examples_train),\n",
    "            \"Train Graph\",\n",
    "            \"Test Graph\",\n",
    "            \"Train the Link Classifier\",\n",
    "        ),\n",
    "        (\n",
    "            \"Model Selection\",\n",
    "            len(examples_model_selection),\n",
    "            \"Train Graph\",\n",
    "            \"Test Graph\",\n",
    "            \"Select the best Link Classifier model\",\n",
    "        ),\n",
    "        (\n",
    "            \"Test set\",\n",
    "            len(examples_test),\n",
    "            \"Test Graph\",\n",
    "            \"Full Graph\",\n",
    "            \"Evaluate the best Link Classifier\",\n",
    "        ),\n",
    "    ],\n",
    "    columns=(\"Split\", \"Number of Examples\", \"Hidden from\", \"Picked from\", \"Use\"),\n",
    ").set_index(\"Split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create random walker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the helper function to generate biased random walks from the given graph with the fixed random walk parameters:\n",
    "\n",
    "* `p` - Random walk parameter \"p\" that defines probability, \"1/p\", of returning to source node\n",
    "* `q` - Random walk parameter \"q\" that defines probability, \"1/q\", for moving to a node away from the source node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.data import BiasedRandomWalk\n",
    "\n",
    "# walk_length = the hop length till where a walk will be continued\n",
    "# walk_num    = the total number of walks of walk length\n",
    "def create_biased_random_walker(graph, walk_num, walk_length):\n",
    "    # parameter settings for \"p\" and \"q\":\n",
    "    p = 1.0\n",
    "    q = 1.0\n",
    "    return BiasedRandomWalk(graph, n=walk_num, length=walk_length, p=p, q=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train Node2Vec, Attri2Vec, GraphSAGE, and GCN by following the same unsupervised learning procedure: we firstly generate a set of short random walks from the given graph and then learn node embeddings from batches of `target, context` pairs collected from random walks. For learning node embeddings, we need to specify the following parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `dimension` - Dimensionality of node embeddings\n",
    "* `walk_number` - Number of walks from each node\n",
    "* `walk_length` - Length of each random walk\n",
    "* `epochs` - The number of epochs to train embedding learning model\n",
    "* `batch_size` - The batch size to train embedding learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consistently set the node embedding dimension to 128 for all algorithms. However, we use different hidden layers to learn node embeddings for different algorithms to exert their respective power. For the remaining parameters, we set them as:\n",
    "\n",
    "|               | Node2Vec | Attri2Vec | GraphSAGE | GCN |\n",
    "|---------------|----------|-----------|-----------|-----|\n",
    "| `walk_number` |    20    |     4     |     1     |  1  |\n",
    "| `walk_length` |     5    |     5     |     5     |  5  |\n",
    "| `epochs`      |    6     |     6     |     6     |  6  |\n",
    "| `batch_size`  |    50    |     50    |    50     |  50 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all algorithms use the same `walk_length`, `batch_size` and `epochs` values, we uniformly set them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_length = 5\n",
    "epochs = 1\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different algorithms, users can find the best parameter setting with the `Model Selection` edge set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec\n",
    "\n",
    "We use Node2Vec [1], to calculate node embeddings. These embeddings are learned in such a way to ensure that nodes that are close in the graph remain close in the embedding space. We train Node2Vec with the Stellargraph Node2Vec components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.data import UnsupervisedSampler\n",
    "from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "from stellargraph.layer import Node2Vec, link_classification\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def node2vec_embedding(graph, name):\n",
    "\n",
    "    # Set the embedding dimension and walk number:\n",
    "    dimension = 128\n",
    "    walk_number = 20\n",
    "\n",
    "    print(f\"Training Node2Vec for '{name}':\")\n",
    "\n",
    "    graph_node_list = list(graph.nodes())\n",
    "\n",
    "    # Create the biased random walker to generate random walks\n",
    "    walker = create_biased_random_walker(graph, walk_number, walk_length)\n",
    "\n",
    "    # Create the unsupervised sampler to sample (target, context) pairs from random walks\n",
    "    unsupervised_samples = UnsupervisedSampler(\n",
    "        graph, nodes=graph_node_list, walker=walker\n",
    "    )\n",
    "\n",
    "    # Define a Node2Vec training generator, which generates batches of training pairs\n",
    "    generator = Node2VecLinkGenerator(graph, batch_size)\n",
    "\n",
    "    # Create the Node2Vec model\n",
    "    node2vec = Node2Vec(dimension, generator=generator)\n",
    "\n",
    "    # Build the model and expose input and output sockets of Node2Vec, for node pair inputs\n",
    "    x_inp, x_out = node2vec.in_out_tensors()\n",
    "\n",
    "    # Use the link_classification function to generate the output of the Node2Vec model\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"dot\"\n",
    "    )(x_out)\n",
    "\n",
    "    # Stack the Node2Vec encoder and prediction layer into a Keras model, and specify the loss\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=2,\n",
    "        use_multiprocessing=False,\n",
    "        workers=4,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Build the model to predict node representations from node ids with the learned Node2Vec model parameters\n",
    "    x_inp_src = x_inp[0]\n",
    "    x_out_src = x_out[0]\n",
    "    embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)\n",
    "\n",
    "    # Get representations for all nodes in ``graph``\n",
    "    node_gen = Node2VecNodeGenerator(graph, batch_size).flow(graph_node_list)\n",
    "    node_embeddings = embedding_model.predict(node_gen, workers=1, verbose=0)\n",
    "\n",
    "    def get_embedding(u):\n",
    "        u_index = graph_node_list.index(u)\n",
    "        return node_embeddings[u_index]\n",
    "\n",
    "    return get_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attri2Vec\n",
    "\n",
    "We use Attri2Vec [2] to calculate node embeddings. Attri2Vec learns node representations through performing a linear/non-linear mapping on node content attributes and simultaneously making nodes sharing similar context nodes in random walks have similar representations. With the node content features are used to learn node embeddings, we wish that Attri2Vec can achieve better link prediction performance than the only structure preserving network embedding algorithm Node2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.mapper import Attri2VecLinkGenerator, Attri2VecNodeGenerator\n",
    "from stellargraph.layer import Attri2Vec\n",
    "\n",
    "\n",
    "def attri2vec_embedding(graph, name):\n",
    "\n",
    "    # Set the embedding dimension and walk number:\n",
    "    dimension = [128]\n",
    "    walk_number = 4\n",
    "\n",
    "    print(f\"Training Attri2Vec for '{name}':\")\n",
    "\n",
    "    graph_node_list = list(graph.nodes())\n",
    "\n",
    "    # Create the biased random walker to generate random walks\n",
    "    walker = create_biased_random_walker(graph, walk_number, walk_length)\n",
    "\n",
    "    # Create the unsupervised sampler to sample (target, context) pairs from random walks\n",
    "    unsupervised_samples = UnsupervisedSampler(\n",
    "        graph, nodes=graph_node_list, walker=walker\n",
    "    )\n",
    "\n",
    "    # Define an Attri2Vec training generator, which generates batches of training pairs\n",
    "    generator = Attri2VecLinkGenerator(graph, batch_size)\n",
    "\n",
    "    # Create the Attri2Vec model\n",
    "    attri2vec = Attri2Vec(\n",
    "        layer_sizes=dimension, generator=generator, bias=False, normalize=None\n",
    "    )\n",
    "\n",
    "    # Build the model and expose input and output sockets of Attri2Vec, for node pair inputs\n",
    "    x_inp, x_out = attri2vec.in_out_tensors()\n",
    "\n",
    "    # Use the link_classification function to generate the output of the Attri2Vec model\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\"\n",
    "    )(x_out)\n",
    "\n",
    "    # Stack the Attri2Vec encoder and prediction layer into a Keras model, and specify the loss\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=2,\n",
    "        use_multiprocessing=False,\n",
    "        workers=1,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Build the model to predict node representations from node features with the learned Attri2Vec model parameters\n",
    "    x_inp_src = x_inp[0]\n",
    "    x_out_src = x_out[0]\n",
    "    embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)\n",
    "\n",
    "    # Get representations for all nodes in ``graph``\n",
    "    node_gen = Attri2VecNodeGenerator(graph, batch_size).flow(graph_node_list)\n",
    "    node_embeddings = embedding_model.predict(node_gen, workers=1, verbose=0)\n",
    "\n",
    "    def get_embedding(u):\n",
    "        u_index = graph_node_list.index(u)\n",
    "        return node_embeddings[u_index]\n",
    "\n",
    "    return get_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSAGE\n",
    "\n",
    "GraphSAGE [3] learns node embeddings for attributed graphs through aggregating neighboring node attributes. The aggregation parameters are learned by encouraging node pairs co-occurring in short random walks to have similar representations. As node attributes are also leveraged, GraphSAGE is expected to perform better than Node2Vec in link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.mapper import GraphSAGELinkGenerator, GraphSAGENodeGenerator\n",
    "from stellargraph.layer import GraphSAGE\n",
    "\n",
    "\n",
    "def graphsage_embedding(graph, name):\n",
    "\n",
    "    # Set the embedding dimensions, the numbers of sampled neighboring nodes and walk number:\n",
    "    dimensions = [128, 128]\n",
    "    num_samples = [10, 5]\n",
    "    walk_number = 1\n",
    "\n",
    "    print(f\"Training GraphSAGE for '{name}':\")\n",
    "\n",
    "    graph_node_list = list(graph.nodes())\n",
    "\n",
    "    # Create the biased random walker to generate random walks\n",
    "    walker = create_biased_random_walker(graph, walk_number, walk_length)\n",
    "\n",
    "    # Create the unsupervised sampler to sample (target, context) pairs from random walks\n",
    "    unsupervised_samples = UnsupervisedSampler(\n",
    "        graph, nodes=graph_node_list, walker=walker\n",
    "    )\n",
    "\n",
    "    # Define a GraphSAGE training generator, which generates batches of training pairs\n",
    "    generator = GraphSAGELinkGenerator(graph, batch_size, num_samples)\n",
    "\n",
    "    # Create the GraphSAGE model\n",
    "    graphsage = GraphSAGE(\n",
    "        layer_sizes=dimensions,\n",
    "        generator=generator,\n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "        normalize=\"l2\",\n",
    "    )\n",
    "\n",
    "    # Build the model and expose input and output sockets of GraphSAGE, for node pair inputs\n",
    "    x_inp, x_out = graphsage.in_out_tensors()\n",
    "\n",
    "    # Use the link_classification function to generate the output of the GraphSAGE model\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\"\n",
    "    )(x_out)\n",
    "\n",
    "    # Stack the GraphSAGE encoder and prediction layer into a Keras model, and specify the loss\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=2,\n",
    "        use_multiprocessing=False,\n",
    "        workers=4,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Build the model to predict node representations from node features with the learned GraphSAGE model parameters\n",
    "    x_inp_src = x_inp[0::2]\n",
    "    x_out_src = x_out[0]\n",
    "    embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)\n",
    "\n",
    "    # Get representations for all nodes in ``graph``\n",
    "    node_gen = GraphSAGENodeGenerator(graph, batch_size, num_samples).flow(\n",
    "        graph_node_list\n",
    "    )\n",
    "    node_embeddings = embedding_model.predict(node_gen, workers=1, verbose=0)\n",
    "\n",
    "    def get_embedding(u):\n",
    "        u_index = graph_node_list.index(u)\n",
    "        return node_embeddings[u_index]\n",
    "\n",
    "    return get_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN\n",
    "\n",
    "GCN [4] learns node embeddings through graph convolution. Traditional GCN relies on node labels as a supervision to perform training. Here, we consider the unsupervised link prediction setting and we try to learn informative GCN node embeddings by making nodes co-occurring in short random walks represented closely, as is performed in training GraphSAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.mapper import FullBatchLinkGenerator, FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN, LinkEmbedding\n",
    "\n",
    "\n",
    "def gcn_embedding(graph, name):\n",
    "\n",
    "    # Set the embedding dimensions and walk number:\n",
    "    dimensions = [128, 128]\n",
    "    walk_number = 1\n",
    "\n",
    "    print(f\"Training GCN for '{name}':\")\n",
    "\n",
    "    graph_node_list = list(graph.nodes())\n",
    "\n",
    "    # Create the biased random walker to generate random walks\n",
    "    walker = create_biased_random_walker(graph, walk_number, walk_length)\n",
    "\n",
    "    # Create the unsupervised sampler to sample (target, context) pairs from random walks\n",
    "    unsupervised_samples = UnsupervisedSampler(\n",
    "        graph, nodes=graph_node_list, walker=walker\n",
    "    )\n",
    "\n",
    "    # Define a GCN training generator, which generates the full batch of training pairs\n",
    "    generator = FullBatchLinkGenerator(graph, method=\"gcn\")\n",
    "\n",
    "    # Create the GCN model\n",
    "    gcn = GCN(\n",
    "        layer_sizes=dimensions,\n",
    "        activations=[\"relu\", \"relu\"],\n",
    "        generator=generator,\n",
    "        dropout=0.3,\n",
    "    )\n",
    "\n",
    "    # Build the model and expose input and output sockets of GCN, for node pair inputs\n",
    "    x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "    # Use the dot product of node embeddings to make node pairs co-occurring in short random walks represented closely\n",
    "    prediction = LinkEmbedding(activation=\"sigmoid\", method=\"ip\")(x_out)\n",
    "    prediction = keras.layers.Reshape((-1,))(prediction)\n",
    "\n",
    "    # Stack the GCN encoder and prediction layer into a Keras model, and specify the loss\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    batches = unsupervised_samples.run(batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "        batch_iter = 1\n",
    "        for batch in batches:\n",
    "            samples = generator.flow(batch[0], targets=batch[1], use_ilocs=True)[0]\n",
    "            [loss, accuracy] = model.train_on_batch(x=samples[0], y=samples[1])\n",
    "            output = (\n",
    "                f\"{batch_iter}/{len(batches)} - loss:\"\n",
    "                + \" {:6.4f}\".format(loss)\n",
    "                + \" - binary_accuracy:\"\n",
    "                + \" {:6.4f}\".format(accuracy)\n",
    "            )\n",
    "            print(\"Tonni = \", output)  # Tonni =  62/384 - loss: 0.6082 - binary_accuracy: 0.4800\n",
    "            if batch_iter == len(batches):\n",
    "                print(output)\n",
    "            else:\n",
    "                print(output, end=\"\\r\")\n",
    "            batch_iter = batch_iter + 1\n",
    "\n",
    "    # Get representations for all nodes in ``graph``\n",
    "    embedding_model = keras.Model(inputs=x_inp, outputs=x_out)\n",
    "    node_embeddings = embedding_model.predict(\n",
    "        generator.flow(list(zip(graph_node_list, graph_node_list)))\n",
    "    )\n",
    "    node_embeddings = node_embeddings[0][:, 0, :]\n",
    "\n",
    "    def get_embedding(u):\n",
    "        u_index = graph_node_list.index(u)\n",
    "        return node_embeddings[u_index]\n",
    "\n",
    "    return get_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the link prediction model\n",
    "\n",
    "There are a few steps involved in using the learned embeddings to perform link prediction:\n",
    "1. We calculate link/edge embeddings for the positive and negative edge samples by applying a binary operator on the embeddings of the source and target nodes of each sampled edge.\n",
    "2. Given the embeddings of the positive and negative examples, we train a logistic regression classifier to predict a binary value indicating whether an edge between two nodes should exist or not.\n",
    "3. We evaluate the performance of the link classifier for each of the 4 operators on the training data with node embeddings calculated on the **Train Graph** (`graph_train`), and select the best classifier.\n",
    "4. The best classifier is then used to calculate scores on the test data with node embeddings trained on the **Train Graph** (`graph_train`).\n",
    "\n",
    "Below are a set of helper functions that let us repeat these steps for each of the binary operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 1. link embeddings\n",
    "def link_examples_to_features(link_examples, transform_node, binary_operator):\n",
    "    return [\n",
    "        binary_operator(transform_node(src), transform_node(dst))\n",
    "        for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "\n",
    "# 2. training classifier\n",
    "def train_link_prediction_model(\n",
    "    link_examples, link_labels, get_embedding, binary_operator\n",
    "):\n",
    "    clf = link_prediction_classifier()\n",
    "    link_features = link_examples_to_features(\n",
    "        link_examples, get_embedding, binary_operator\n",
    "    )\n",
    "    clf.fit(link_features, link_labels)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def link_prediction_classifier(max_iter=5000):\n",
    "    lr_clf = LogisticRegressionCV(Cs=10, cv=10, scoring=\"roc_auc\", max_iter=max_iter)\n",
    "    return Pipeline(steps=[(\"sc\", StandardScaler()), (\"clf\", lr_clf)])\n",
    "\n",
    "\n",
    "# 3. and 4. evaluate classifier\n",
    "def evaluate_link_prediction_model(\n",
    "    clf, link_examples_test, link_labels_test, get_embedding, binary_operator\n",
    "):\n",
    "    link_features_test = link_examples_to_features(\n",
    "        link_examples_test, get_embedding, binary_operator\n",
    "    )\n",
    "    roc_score = evaluate_roc_auc(clf, link_features_test, link_labels_test)\n",
    "    ap_score = evaluate_ap(clf, link_features_test, link_labels_test)\n",
    "    return roc_score, ap_score\n",
    "\n",
    "\n",
    "def evaluate_roc_auc(clf, link_features, link_labels):\n",
    "    predicted = clf.predict_proba(link_features)\n",
    "\n",
    "    # check which class corresponds to positive links\n",
    "    positive_column = list(clf.classes_).index(1)\n",
    "    return roc_auc_score(link_labels, predicted[:, positive_column])\n",
    "\n",
    "# Tonni: To get AP score\n",
    "def evaluate_ap(clf, link_features, link_labels):\n",
    "    predicted = clf.predict_proba(link_features)\n",
    "\n",
    "    # check which class corresponds to positive links\n",
    "    positive_column = list(clf.classes_).index(1)\n",
    "    return average_precision_score(link_labels, predicted[:, positive_column])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider 4 different operators: \n",
    "\n",
    "* *Hadamard*\n",
    "* $L_1$\n",
    "* $L_2$\n",
    "* *average*\n",
    "\n",
    "The paper [1] provides a detailed description of these operators. All operators produce link embeddings that have equal dimensionality to the input node embeddings (128 dimensions for our example). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operator_hadamard(u, v):\n",
    "    return u * v\n",
    "\n",
    "\n",
    "def operator_l1(u, v):\n",
    "    return np.abs(u - v)\n",
    "\n",
    "\n",
    "def operator_l2(u, v):\n",
    "    return (u - v) ** 2\n",
    "\n",
    "\n",
    "def operator_avg(u, v):\n",
    "    return (u + v) / 2.0\n",
    "\n",
    "\n",
    "def run_link_prediction(binary_operator, embedding_train):\n",
    "    clf = train_link_prediction_model(\n",
    "        examples_train, labels_train, embedding_train, binary_operator\n",
    "    )\n",
    "    score = evaluate_link_prediction_model(\n",
    "        clf,\n",
    "        examples_model_selection,\n",
    "        labels_model_selection,\n",
    "        embedding_train,\n",
    "        binary_operator,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"classifier\": clf,\n",
    "        \"binary_operator\": binary_operator,\n",
    "        \"score\": score,\n",
    "    }\n",
    "\n",
    "\n",
    "binary_operators = [operator_hadamard, operator_l1, operator_l2, operator_avg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the link model with the specified embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(embedding, name):\n",
    "\n",
    "    embedding_train = embedding(graph_train, \"Train Graph\")\n",
    "\n",
    "    # Train the link classification model with the learned embedding\n",
    "    results = [run_link_prediction(op, embedding_train) for op in binary_operators]\n",
    "    best_result = max(results, key=lambda result: result[\"score\"])\n",
    "    print(\n",
    "        f\"\\nBest result with '{name}' embeddings from '{best_result['binary_operator'].__name__}'\"\n",
    "    )\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            [(result[\"binary_operator\"].__name__, result[\"score\"]) for result in results],\n",
    "            columns=(\"name\", \"ROC AUC\"),\n",
    "        ).set_index(\"name\")\n",
    "    )\n",
    "\n",
    "    # Evaluate the best model using the test set\n",
    "    test_score = evaluate_link_prediction_model(\n",
    "        best_result[\"classifier\"],\n",
    "        examples_test,\n",
    "        labels_test,\n",
    "        embedding_train,\n",
    "        best_result[\"binary_operator\"],\n",
    "    )\n",
    "\n",
    "    return test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the link prediction results for Node2Vec, Attri2Vec, GraphSAGE and GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Node2Vec link prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Node2Vec for 'Train Graph':\n",
      "link_classification: using 'dot' method to combine node embeddings into edge embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7684/7684 - 26s - loss: 0.5557 - binary_accuracy: 0.6707 - 26s/epoch - 3ms/step\n",
      "\n",
      "Best result with 'Node2Vec' embeddings from 'operator_avg'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>operator_hadamard</th>\n",
       "      <td>(0.566605504587156, 0.5899722892865835)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_l1</th>\n",
       "      <td>(0.5262385321100918, 0.5407857277221773)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_l2</th>\n",
       "      <td>(0.5502385321100918, 0.5621738577550801)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_avg</th>\n",
       "      <td>(0.5867155963302753, 0.6190518563542797)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ROC AUC\n",
       "name                                                       \n",
       "operator_hadamard   (0.566605504587156, 0.5899722892865835)\n",
       "operator_l1        (0.5262385321100918, 0.5407857277221773)\n",
       "operator_l2        (0.5502385321100918, 0.5621738577550801)\n",
       "operator_avg       (0.5867155963302753, 0.6190518563542797)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "node2vec_result = train_and_evaluate(node2vec_embedding, \"Node2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Attri2Vec link prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Attri2Vec for 'Train Graph':\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "1537/1537 - 4s - loss: 0.6992 - binary_accuracy: 0.5329 - 4s/epoch - 3ms/step\n",
      "\n",
      "Best result with 'Attri2Vec' embeddings from 'operator_l2'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>operator_hadamard</th>\n",
       "      <td>(0.5370275229357798, 0.5948197574849609)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_l1</th>\n",
       "      <td>(0.6244403669724771, 0.6782942010320367)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_l2</th>\n",
       "      <td>(0.6581284403669726, 0.6398877822696833)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_avg</th>\n",
       "      <td>(0.5008440366972478, 0.5386264255969719)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ROC AUC\n",
       "name                                                       \n",
       "operator_hadamard  (0.5370275229357798, 0.5948197574849609)\n",
       "operator_l1        (0.6244403669724771, 0.6782942010320367)\n",
       "operator_l2        (0.6581284403669726, 0.6398877822696833)\n",
       "operator_avg       (0.5008440366972478, 0.5386264255969719)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attri2vec_result = train_and_evaluate(attri2vec_embedding, \"Attri2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get GraphSAGE link prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GraphSAGE for 'Train Graph':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonni/opt/anaconda3/envs/sg/lib/python3.8/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "385/385 - 20s - loss: 0.5582 - binary_accuracy: 0.7516 - 20s/epoch - 53ms/step\n",
      "\n",
      "Best result with 'GraphSAGE' embeddings from 'operator_l2'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>operator_hadamard</th>\n",
       "      <td>(0.8998899082568808, 0.9105648960349663)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_l1</th>\n",
       "      <td>(0.897467889908257, 0.9135980025339935)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_l2</th>\n",
       "      <td>(0.9003302752293579, 0.9140604980739123)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_avg</th>\n",
       "      <td>(0.465834862385321, 0.5220822111254122)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ROC AUC\n",
       "name                                                       \n",
       "operator_hadamard  (0.8998899082568808, 0.9105648960349663)\n",
       "operator_l1         (0.897467889908257, 0.9135980025339935)\n",
       "operator_l2        (0.9003302752293579, 0.9140604980739123)\n",
       "operator_avg        (0.465834862385321, 0.5220822111254122)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphsage_result = train_and_evaluate(graphsage_embedding, \"GraphSAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get GCN link prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN for 'Train Graph':\n",
      "Using GCN (local pooling) filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n",
      "Tonni =  1/385 - loss: 0.6658 - binary_accuracy: 0.5800\n",
      "Tonni =  2/385 - loss: 0.6889 - binary_accuracy: 0.4600\n",
      "Tonni =  3/385 - loss: 0.6887 - binary_accuracy: 0.4400\n",
      "Tonni =  4/385 - loss: 0.6980 - binary_accuracy: 0.3600\n",
      "Tonni =  5/385 - loss: 0.6448 - binary_accuracy: 0.6200\n",
      "Tonni =  6/385 - loss: 0.6813 - binary_accuracy: 0.4800\n",
      "Tonni =  7/385 - loss: 0.6929 - binary_accuracy: 0.4000\n",
      "Tonni =  8/385 - loss: 0.6676 - binary_accuracy: 0.5000\n",
      "Tonni =  9/385 - loss: 0.6889 - binary_accuracy: 0.4400\n",
      "Tonni =  10/385 - loss: 0.6799 - binary_accuracy: 0.4600\n",
      "Tonni =  11/385 - loss: 0.7093 - binary_accuracy: 0.4000\n",
      "Tonni =  12/385 - loss: 0.6614 - binary_accuracy: 0.5800\n",
      "Tonni =  13/385 - loss: 0.6753 - binary_accuracy: 0.5000\n",
      "Tonni =  14/385 - loss: 0.6539 - binary_accuracy: 0.5800\n",
      "Tonni =  15/385 - loss: 0.6693 - binary_accuracy: 0.5600\n",
      "Tonni =  16/385 - loss: 0.6655 - binary_accuracy: 0.5400\n",
      "Tonni =  17/385 - loss: 0.6689 - binary_accuracy: 0.5000\n",
      "Tonni =  18/385 - loss: 0.6573 - binary_accuracy: 0.5400\n",
      "Tonni =  19/385 - loss: 0.6844 - binary_accuracy: 0.4600\n",
      "Tonni =  20/385 - loss: 0.6894 - binary_accuracy: 0.4200\n",
      "Tonni =  21/385 - loss: 0.6624 - binary_accuracy: 0.5000\n",
      "Tonni =  22/385 - loss: 0.7047 - binary_accuracy: 0.4000\n",
      "Tonni =  23/385 - loss: 0.6749 - binary_accuracy: 0.4200\n",
      "Tonni =  24/385 - loss: 0.6717 - binary_accuracy: 0.5200\n",
      "Tonni =  25/385 - loss: 0.7026 - binary_accuracy: 0.3800\n",
      "Tonni =  26/385 - loss: 0.6758 - binary_accuracy: 0.4400\n",
      "Tonni =  27/385 - loss: 0.6579 - binary_accuracy: 0.5200\n",
      "Tonni =  28/385 - loss: 0.6433 - binary_accuracy: 0.5800\n",
      "Tonni =  29/385 - loss: 0.6706 - binary_accuracy: 0.5000\n",
      "Tonni =  30/385 - loss: 0.6541 - binary_accuracy: 0.5200\n",
      "Tonni =  31/385 - loss: 0.6742 - binary_accuracy: 0.4800\n",
      "Tonni =  32/385 - loss: 0.6497 - binary_accuracy: 0.4800\n",
      "Tonni =  33/385 - loss: 0.6430 - binary_accuracy: 0.6000\n",
      "Tonni =  34/385 - loss: 0.6668 - binary_accuracy: 0.5400\n",
      "Tonni =  35/385 - loss: 0.6272 - binary_accuracy: 0.5600\n",
      "Tonni =  36/385 - loss: 0.6260 - binary_accuracy: 0.6000\n",
      "Tonni =  37/385 - loss: 0.6910 - binary_accuracy: 0.4000\n",
      "Tonni =  38/385 - loss: 0.6621 - binary_accuracy: 0.4400\n",
      "Tonni =  39/385 - loss: 0.6354 - binary_accuracy: 0.5200\n",
      "Tonni =  40/385 - loss: 0.6605 - binary_accuracy: 0.4800\n",
      "Tonni =  41/385 - loss: 0.6760 - binary_accuracy: 0.5000\n",
      "Tonni =  42/385 - loss: 0.6896 - binary_accuracy: 0.5400\n",
      "Tonni =  43/385 - loss: 0.6313 - binary_accuracy: 0.5800\n",
      "Tonni =  44/385 - loss: 0.6143 - binary_accuracy: 0.5600\n",
      "Tonni =  45/385 - loss: 0.6224 - binary_accuracy: 0.5600\n",
      "Tonni =  46/385 - loss: 0.6254 - binary_accuracy: 0.5000\n",
      "Tonni =  47/385 - loss: 0.6515 - binary_accuracy: 0.4200\n",
      "Tonni =  48/385 - loss: 0.6185 - binary_accuracy: 0.5000\n",
      "Tonni =  49/385 - loss: 0.5776 - binary_accuracy: 0.6600\n",
      "Tonni =  50/385 - loss: 0.6480 - binary_accuracy: 0.5000\n",
      "Tonni =  51/385 - loss: 0.6114 - binary_accuracy: 0.5800\n",
      "Tonni =  52/385 - loss: 0.6773 - binary_accuracy: 0.4600\n",
      "Tonni =  53/385 - loss: 0.6327 - binary_accuracy: 0.5800\n",
      "Tonni =  54/385 - loss: 0.6090 - binary_accuracy: 0.5400\n",
      "Tonni =  55/385 - loss: 0.6563 - binary_accuracy: 0.4800\n",
      "Tonni =  56/385 - loss: 0.6682 - binary_accuracy: 0.5000\n",
      "Tonni =  57/385 - loss: 0.6113 - binary_accuracy: 0.5200\n",
      "Tonni =  58/385 - loss: 0.6550 - binary_accuracy: 0.4600\n",
      "Tonni =  59/385 - loss: 0.6568 - binary_accuracy: 0.4400\n",
      "Tonni =  60/385 - loss: 0.6271 - binary_accuracy: 0.5000\n",
      "Tonni =  61/385 - loss: 0.6567 - binary_accuracy: 0.4600\n",
      "Tonni =  62/385 - loss: 0.5949 - binary_accuracy: 0.5400\n",
      "Tonni =  63/385 - loss: 0.6260 - binary_accuracy: 0.4800\n",
      "Tonni =  64/385 - loss: 0.6567 - binary_accuracy: 0.4600\n",
      "Tonni =  65/385 - loss: 0.5813 - binary_accuracy: 0.6400\n",
      "Tonni =  66/385 - loss: 0.5960 - binary_accuracy: 0.5600\n",
      "Tonni =  67/385 - loss: 0.6169 - binary_accuracy: 0.4800\n",
      "Tonni =  68/385 - loss: 0.5590 - binary_accuracy: 0.5600\n",
      "Tonni =  69/385 - loss: 0.5674 - binary_accuracy: 0.6200\n",
      "Tonni =  70/385 - loss: 0.6284 - binary_accuracy: 0.5000\n",
      "Tonni =  71/385 - loss: 0.6814 - binary_accuracy: 0.3600\n",
      "Tonni =  72/385 - loss: 0.6083 - binary_accuracy: 0.4800\n",
      "Tonni =  73/385 - loss: 0.5766 - binary_accuracy: 0.5600\n",
      "Tonni =  74/385 - loss: 0.5958 - binary_accuracy: 0.5800\n",
      "Tonni =  75/385 - loss: 0.6452 - binary_accuracy: 0.4600\n",
      "Tonni =  76/385 - loss: 0.6829 - binary_accuracy: 0.3800\n",
      "Tonni =  77/385 - loss: 0.5869 - binary_accuracy: 0.5000\n",
      "Tonni =  78/385 - loss: 0.6408 - binary_accuracy: 0.4800\n",
      "Tonni =  79/385 - loss: 0.6466 - binary_accuracy: 0.4400\n",
      "Tonni =  80/385 - loss: 0.6975 - binary_accuracy: 0.3800\n",
      "Tonni =  81/385 - loss: 0.5959 - binary_accuracy: 0.5800\n",
      "Tonni =  82/385 - loss: 0.6100 - binary_accuracy: 0.4400\n",
      "Tonni =  83/385 - loss: 0.5993 - binary_accuracy: 0.4800\n",
      "Tonni =  84/385 - loss: 0.6975 - binary_accuracy: 0.4200\n",
      "Tonni =  85/385 - loss: 0.6287 - binary_accuracy: 0.4400\n",
      "Tonni =  86/385 - loss: 0.5852 - binary_accuracy: 0.5600\n",
      "Tonni =  87/385 - loss: 0.5159 - binary_accuracy: 0.6400\n",
      "Tonni =  88/385 - loss: 0.5902 - binary_accuracy: 0.5000\n",
      "Tonni =  89/385 - loss: 0.5640 - binary_accuracy: 0.5800\n",
      "Tonni =  90/385 - loss: 0.5997 - binary_accuracy: 0.5600\n",
      "Tonni =  91/385 - loss: 0.6314 - binary_accuracy: 0.4000\n",
      "Tonni =  92/385 - loss: 0.5677 - binary_accuracy: 0.5000\n",
      "Tonni =  93/385 - loss: 0.5987 - binary_accuracy: 0.5000\n",
      "Tonni =  94/385 - loss: 0.6293 - binary_accuracy: 0.5200\n",
      "Tonni =  95/385 - loss: 0.6468 - binary_accuracy: 0.4000\n",
      "Tonni =  96/385 - loss: 0.5786 - binary_accuracy: 0.6000\n",
      "Tonni =  97/385 - loss: 0.5446 - binary_accuracy: 0.5800\n",
      "Tonni =  98/385 - loss: 0.5824 - binary_accuracy: 0.5200\n",
      "Tonni =  99/385 - loss: 0.6678 - binary_accuracy: 0.4200\n",
      "Tonni =  100/385 - loss: 0.6226 - binary_accuracy: 0.4000\n",
      "Tonni =  101/385 - loss: 0.5800 - binary_accuracy: 0.5800\n",
      "Tonni =  102/385 - loss: 0.5744 - binary_accuracy: 0.5000\n",
      "Tonni =  103/385 - loss: 0.7483 - binary_accuracy: 0.4400\n",
      "Tonni =  104/385 - loss: 0.6353 - binary_accuracy: 0.4600\n",
      "Tonni =  105/385 - loss: 0.6091 - binary_accuracy: 0.5800\n",
      "Tonni =  106/385 - loss: 0.6550 - binary_accuracy: 0.5200\n",
      "Tonni =  107/385 - loss: 0.5148 - binary_accuracy: 0.6600\n",
      "Tonni =  108/385 - loss: 0.5847 - binary_accuracy: 0.5200\n",
      "Tonni =  109/385 - loss: 0.5953 - binary_accuracy: 0.4600\n",
      "Tonni =  110/385 - loss: 0.5987 - binary_accuracy: 0.4800\n",
      "Tonni =  111/385 - loss: 0.6727 - binary_accuracy: 0.6000\n",
      "Tonni =  112/385 - loss: 0.6486 - binary_accuracy: 0.3800\n",
      "Tonni =  113/385 - loss: 0.5734 - binary_accuracy: 0.5200\n",
      "Tonni =  114/385 - loss: 0.5599 - binary_accuracy: 0.5600\n",
      "Tonni =  115/385 - loss: 0.5431 - binary_accuracy: 0.5800\n",
      "Tonni =  116/385 - loss: 0.5452 - binary_accuracy: 0.6800\n",
      "Tonni =  117/385 - loss: 0.6014 - binary_accuracy: 0.4400\n",
      "Tonni =  118/385 - loss: 0.5484 - binary_accuracy: 0.5600\n",
      "Tonni =  119/385 - loss: 0.5599 - binary_accuracy: 0.5600\n",
      "Tonni =  120/385 - loss: 0.5948 - binary_accuracy: 0.5000\n",
      "Tonni =  121/385 - loss: 0.6106 - binary_accuracy: 0.6000\n",
      "Tonni =  122/385 - loss: 0.5815 - binary_accuracy: 0.5200\n",
      "Tonni =  123/385 - loss: 0.6638 - binary_accuracy: 0.3800\n",
      "Tonni =  124/385 - loss: 0.5846 - binary_accuracy: 0.5400\n",
      "Tonni =  125/385 - loss: 0.6138 - binary_accuracy: 0.5000\n",
      "Tonni =  126/385 - loss: 0.5976 - binary_accuracy: 0.4600\n",
      "Tonni =  127/385 - loss: 0.5655 - binary_accuracy: 0.5600\n",
      "Tonni =  128/385 - loss: 0.5705 - binary_accuracy: 0.5000\n",
      "Tonni =  129/385 - loss: 0.6463 - binary_accuracy: 0.4800\n",
      "Tonni =  130/385 - loss: 0.5530 - binary_accuracy: 0.5400\n",
      "Tonni =  131/385 - loss: 0.5329 - binary_accuracy: 0.4800\n",
      "Tonni =  132/385 - loss: 0.5795 - binary_accuracy: 0.5400\n",
      "Tonni =  133/385 - loss: 0.6381 - binary_accuracy: 0.4600\n",
      "Tonni =  134/385 - loss: 0.7496 - binary_accuracy: 0.4200\n",
      "Tonni =  135/385 - loss: 0.5393 - binary_accuracy: 0.5200\n",
      "Tonni =  136/385 - loss: 0.6331 - binary_accuracy: 0.5200\n",
      "Tonni =  137/385 - loss: 0.6513 - binary_accuracy: 0.4400\n",
      "Tonni =  138/385 - loss: 0.6147 - binary_accuracy: 0.5200\n",
      "Tonni =  139/385 - loss: 0.6081 - binary_accuracy: 0.5000\n",
      "Tonni =  140/385 - loss: 0.6199 - binary_accuracy: 0.5800\n",
      "Tonni =  141/385 - loss: 0.4722 - binary_accuracy: 0.6200\n",
      "Tonni =  142/385 - loss: 0.5884 - binary_accuracy: 0.4800\n",
      "Tonni =  143/385 - loss: 0.5985 - binary_accuracy: 0.4800\n",
      "Tonni =  144/385 - loss: 0.6216 - binary_accuracy: 0.4800\n",
      "Tonni =  145/385 - loss: 0.5730 - binary_accuracy: 0.5200\n",
      "Tonni =  146/385 - loss: 0.5228 - binary_accuracy: 0.5400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonni =  147/385 - loss: 0.5724 - binary_accuracy: 0.4800\n",
      "Tonni =  148/385 - loss: 0.5878 - binary_accuracy: 0.5000\n",
      "Tonni =  149/385 - loss: 0.6745 - binary_accuracy: 0.4600\n",
      "Tonni =  150/385 - loss: 0.6015 - binary_accuracy: 0.4400\n",
      "Tonni =  151/385 - loss: 0.5721 - binary_accuracy: 0.5000\n",
      "Tonni =  152/385 - loss: 0.4923 - binary_accuracy: 0.6600\n",
      "Tonni =  153/385 - loss: 0.6002 - binary_accuracy: 0.4600\n",
      "Tonni =  154/385 - loss: 0.5834 - binary_accuracy: 0.4800\n",
      "Tonni =  155/385 - loss: 0.6140 - binary_accuracy: 0.4800\n",
      "Tonni =  156/385 - loss: 0.5819 - binary_accuracy: 0.5800\n",
      "Tonni =  157/385 - loss: 0.5839 - binary_accuracy: 0.5200\n",
      "Tonni =  158/385 - loss: 0.6709 - binary_accuracy: 0.4200\n",
      "Tonni =  159/385 - loss: 0.5249 - binary_accuracy: 0.6200\n",
      "Tonni =  160/385 - loss: 0.6100 - binary_accuracy: 0.4000\n",
      "Tonni =  161/385 - loss: 0.6798 - binary_accuracy: 0.4600\n",
      "Tonni =  162/385 - loss: 0.5878 - binary_accuracy: 0.5200\n",
      "Tonni =  163/385 - loss: 0.6202 - binary_accuracy: 0.5000\n",
      "Tonni =  164/385 - loss: 0.6413 - binary_accuracy: 0.4000\n",
      "Tonni =  165/385 - loss: 0.5635 - binary_accuracy: 0.5600\n",
      "Tonni =  166/385 - loss: 0.5444 - binary_accuracy: 0.5200\n",
      "Tonni =  167/385 - loss: 0.6132 - binary_accuracy: 0.5000\n",
      "Tonni =  168/385 - loss: 0.6698 - binary_accuracy: 0.5000\n",
      "Tonni =  169/385 - loss: 0.5309 - binary_accuracy: 0.5000\n",
      "Tonni =  170/385 - loss: 0.5237 - binary_accuracy: 0.5800\n",
      "Tonni =  171/385 - loss: 0.6628 - binary_accuracy: 0.3800\n",
      "Tonni =  172/385 - loss: 0.5637 - binary_accuracy: 0.6400\n",
      "Tonni =  173/385 - loss: 0.6028 - binary_accuracy: 0.5000\n",
      "Tonni =  174/385 - loss: 0.5693 - binary_accuracy: 0.5000\n",
      "Tonni =  175/385 - loss: 0.5618 - binary_accuracy: 0.5000\n",
      "Tonni =  176/385 - loss: 0.5331 - binary_accuracy: 0.5000\n",
      "Tonni =  177/385 - loss: 0.5542 - binary_accuracy: 0.5000\n",
      "Tonni =  178/385 - loss: 0.5202 - binary_accuracy: 0.5400\n",
      "Tonni =  179/385 - loss: 0.5950 - binary_accuracy: 0.4600\n",
      "Tonni =  180/385 - loss: 0.5529 - binary_accuracy: 0.5000\n",
      "Tonni =  181/385 - loss: 0.5639 - binary_accuracy: 0.5200\n",
      "Tonni =  182/385 - loss: 0.6199 - binary_accuracy: 0.5400\n",
      "Tonni =  183/385 - loss: 0.6105 - binary_accuracy: 0.4800\n",
      "Tonni =  184/385 - loss: 0.5828 - binary_accuracy: 0.5800\n",
      "Tonni =  185/385 - loss: 0.5848 - binary_accuracy: 0.4200\n",
      "Tonni =  186/385 - loss: 0.5382 - binary_accuracy: 0.5400\n",
      "Tonni =  187/385 - loss: 0.6036 - binary_accuracy: 0.4200\n",
      "Tonni =  188/385 - loss: 0.5945 - binary_accuracy: 0.3800\n",
      "Tonni =  189/385 - loss: 0.6830 - binary_accuracy: 0.5000\n",
      "Tonni =  190/385 - loss: 0.5702 - binary_accuracy: 0.5600\n",
      "Tonni =  191/385 - loss: 0.5892 - binary_accuracy: 0.5000\n",
      "Tonni =  192/385 - loss: 0.5454 - binary_accuracy: 0.5400\n",
      "Tonni =  193/385 - loss: 0.5780 - binary_accuracy: 0.5200\n",
      "Tonni =  194/385 - loss: 0.6276 - binary_accuracy: 0.5600\n",
      "Tonni =  195/385 - loss: 0.6429 - binary_accuracy: 0.3600\n",
      "Tonni =  196/385 - loss: 0.5205 - binary_accuracy: 0.6600\n",
      "Tonni =  197/385 - loss: 0.5949 - binary_accuracy: 0.5200\n",
      "Tonni =  198/385 - loss: 0.7573 - binary_accuracy: 0.4400\n",
      "Tonni =  199/385 - loss: 0.5700 - binary_accuracy: 0.4600\n",
      "Tonni =  200/385 - loss: 0.5264 - binary_accuracy: 0.6000\n",
      "Tonni =  201/385 - loss: 0.5764 - binary_accuracy: 0.4800\n",
      "Tonni =  202/385 - loss: 0.5085 - binary_accuracy: 0.6200\n",
      "Tonni =  203/385 - loss: 0.5814 - binary_accuracy: 0.5200\n",
      "Tonni =  204/385 - loss: 0.6511 - binary_accuracy: 0.5200\n",
      "Tonni =  205/385 - loss: 0.6025 - binary_accuracy: 0.4800\n",
      "Tonni =  206/385 - loss: 0.6398 - binary_accuracy: 0.5000\n",
      "Tonni =  207/385 - loss: 0.6386 - binary_accuracy: 0.5600\n",
      "Tonni =  208/385 - loss: 0.5406 - binary_accuracy: 0.5200\n",
      "Tonni =  209/385 - loss: 0.5588 - binary_accuracy: 0.5200\n",
      "Tonni =  210/385 - loss: 0.6988 - binary_accuracy: 0.4600\n",
      "Tonni =  211/385 - loss: 0.5892 - binary_accuracy: 0.4600\n",
      "Tonni =  212/385 - loss: 0.5283 - binary_accuracy: 0.5600\n",
      "Tonni =  213/385 - loss: 0.5519 - binary_accuracy: 0.4800\n",
      "Tonni =  214/385 - loss: 0.5320 - binary_accuracy: 0.6000\n",
      "Tonni =  215/385 - loss: 0.6087 - binary_accuracy: 0.4800\n",
      "Tonni =  216/385 - loss: 0.5834 - binary_accuracy: 0.5400\n",
      "Tonni =  217/385 - loss: 0.6561 - binary_accuracy: 0.6000\n",
      "Tonni =  218/385 - loss: 0.5547 - binary_accuracy: 0.4800\n",
      "Tonni =  219/385 - loss: 0.5492 - binary_accuracy: 0.5600\n",
      "Tonni =  220/385 - loss: 0.5312 - binary_accuracy: 0.5600\n",
      "Tonni =  221/385 - loss: 0.5073 - binary_accuracy: 0.5800\n",
      "Tonni =  222/385 - loss: 0.5572 - binary_accuracy: 0.4800\n",
      "Tonni =  223/385 - loss: 0.4841 - binary_accuracy: 0.6600\n",
      "Tonni =  224/385 - loss: 0.5534 - binary_accuracy: 0.5600\n",
      "Tonni =  225/385 - loss: 0.6090 - binary_accuracy: 0.4600\n",
      "Tonni =  226/385 - loss: 0.5149 - binary_accuracy: 0.5600\n",
      "Tonni =  227/385 - loss: 0.6414 - binary_accuracy: 0.4800\n",
      "Tonni =  228/385 - loss: 0.6253 - binary_accuracy: 0.5800\n",
      "Tonni =  229/385 - loss: 0.5307 - binary_accuracy: 0.5200\n",
      "Tonni =  230/385 - loss: 0.4824 - binary_accuracy: 0.6000\n",
      "Tonni =  231/385 - loss: 0.5291 - binary_accuracy: 0.6000\n",
      "Tonni =  232/385 - loss: 0.4977 - binary_accuracy: 0.6600\n",
      "Tonni =  233/385 - loss: 0.5680 - binary_accuracy: 0.5200\n",
      "Tonni =  234/385 - loss: 0.5251 - binary_accuracy: 0.5600\n",
      "Tonni =  235/385 - loss: 0.5156 - binary_accuracy: 0.5600\n",
      "Tonni =  236/385 - loss: 0.4889 - binary_accuracy: 0.5400\n",
      "Tonni =  237/385 - loss: 0.6854 - binary_accuracy: 0.4600\n",
      "Tonni =  238/385 - loss: 0.5832 - binary_accuracy: 0.5200\n",
      "Tonni =  239/385 - loss: 0.5564 - binary_accuracy: 0.5800\n",
      "Tonni =  240/385 - loss: 0.5066 - binary_accuracy: 0.5800\n",
      "Tonni =  241/385 - loss: 0.5584 - binary_accuracy: 0.5600\n",
      "Tonni =  242/385 - loss: 0.5108 - binary_accuracy: 0.6000\n",
      "Tonni =  243/385 - loss: 0.6232 - binary_accuracy: 0.5200\n",
      "Tonni =  244/385 - loss: 0.5371 - binary_accuracy: 0.6600\n",
      "Tonni =  245/385 - loss: 0.5311 - binary_accuracy: 0.5200\n",
      "Tonni =  246/385 - loss: 0.5682 - binary_accuracy: 0.5600\n",
      "Tonni =  247/385 - loss: 0.7779 - binary_accuracy: 0.4400\n",
      "Tonni =  248/385 - loss: 0.4870 - binary_accuracy: 0.6000\n",
      "Tonni =  249/385 - loss: 0.5993 - binary_accuracy: 0.5400\n",
      "Tonni =  250/385 - loss: 0.5223 - binary_accuracy: 0.5000\n",
      "Tonni =  251/385 - loss: 0.5601 - binary_accuracy: 0.5600\n",
      "Tonni =  252/385 - loss: 0.6044 - binary_accuracy: 0.6000\n",
      "Tonni =  253/385 - loss: 0.5370 - binary_accuracy: 0.5200\n",
      "Tonni =  254/385 - loss: 0.5878 - binary_accuracy: 0.5000\n",
      "Tonni =  255/385 - loss: 0.5840 - binary_accuracy: 0.6000\n",
      "Tonni =  256/385 - loss: 0.6164 - binary_accuracy: 0.4200\n",
      "Tonni =  257/385 - loss: 0.5459 - binary_accuracy: 0.5800\n",
      "Tonni =  258/385 - loss: 0.6291 - binary_accuracy: 0.5400\n",
      "Tonni =  259/385 - loss: 0.5437 - binary_accuracy: 0.6400\n",
      "Tonni =  260/385 - loss: 0.5381 - binary_accuracy: 0.6800\n",
      "Tonni =  261/385 - loss: 0.6124 - binary_accuracy: 0.4800\n",
      "Tonni =  262/385 - loss: 0.6777 - binary_accuracy: 0.5600\n",
      "Tonni =  263/385 - loss: 0.5353 - binary_accuracy: 0.5000\n",
      "Tonni =  264/385 - loss: 0.5511 - binary_accuracy: 0.6000\n",
      "Tonni =  265/385 - loss: 0.5584 - binary_accuracy: 0.6000\n",
      "Tonni =  266/385 - loss: 0.5999 - binary_accuracy: 0.5600\n",
      "Tonni =  267/385 - loss: 0.5064 - binary_accuracy: 0.7200\n",
      "Tonni =  268/385 - loss: 0.5580 - binary_accuracy: 0.4800\n",
      "Tonni =  269/385 - loss: 0.5666 - binary_accuracy: 0.5600\n",
      "Tonni =  270/385 - loss: 0.6050 - binary_accuracy: 0.4200\n",
      "Tonni =  271/385 - loss: 0.6378 - binary_accuracy: 0.6200\n",
      "Tonni =  272/385 - loss: 0.5293 - binary_accuracy: 0.5400\n",
      "Tonni =  273/385 - loss: 0.5843 - binary_accuracy: 0.5800\n",
      "Tonni =  274/385 - loss: 0.5684 - binary_accuracy: 0.5200\n",
      "Tonni =  275/385 - loss: 0.4616 - binary_accuracy: 0.7800\n",
      "Tonni =  276/385 - loss: 0.4486 - binary_accuracy: 0.7800\n",
      "Tonni =  277/385 - loss: 0.6068 - binary_accuracy: 0.5000\n",
      "Tonni =  278/385 - loss: 0.5924 - binary_accuracy: 0.5800\n",
      "Tonni =  279/385 - loss: 0.7372 - binary_accuracy: 0.3600\n",
      "Tonni =  280/385 - loss: 0.5309 - binary_accuracy: 0.6200\n",
      "Tonni =  281/385 - loss: 0.5107 - binary_accuracy: 0.5400\n",
      "Tonni =  282/385 - loss: 0.6202 - binary_accuracy: 0.5200\n",
      "Tonni =  283/385 - loss: 0.5806 - binary_accuracy: 0.4400\n",
      "Tonni =  284/385 - loss: 0.5411 - binary_accuracy: 0.5400\n",
      "Tonni =  285/385 - loss: 0.5329 - binary_accuracy: 0.5800\n",
      "Tonni =  286/385 - loss: 0.5121 - binary_accuracy: 0.6000\n",
      "Tonni =  287/385 - loss: 0.5369 - binary_accuracy: 0.5200\n",
      "Tonni =  288/385 - loss: 0.5582 - binary_accuracy: 0.6000\n",
      "Tonni =  289/385 - loss: 0.5596 - binary_accuracy: 0.6400\n",
      "Tonni =  290/385 - loss: 0.6761 - binary_accuracy: 0.4400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonni =  291/385 - loss: 0.5796 - binary_accuracy: 0.5000\n",
      "Tonni =  292/385 - loss: 0.5258 - binary_accuracy: 0.5800\n",
      "Tonni =  293/385 - loss: 0.6140 - binary_accuracy: 0.5200\n",
      "Tonni =  294/385 - loss: 0.6988 - binary_accuracy: 0.5200\n",
      "Tonni =  295/385 - loss: 0.5630 - binary_accuracy: 0.4200\n",
      "Tonni =  296/385 - loss: 0.4976 - binary_accuracy: 0.6400\n",
      "Tonni =  297/385 - loss: 0.4865 - binary_accuracy: 0.6200\n",
      "Tonni =  298/385 - loss: 0.5672 - binary_accuracy: 0.5600\n",
      "Tonni =  299/385 - loss: 0.6232 - binary_accuracy: 0.6000\n",
      "Tonni =  300/385 - loss: 0.4974 - binary_accuracy: 0.6200\n",
      "Tonni =  301/385 - loss: 0.6534 - binary_accuracy: 0.5600\n",
      "Tonni =  302/385 - loss: 0.5518 - binary_accuracy: 0.6000\n",
      "Tonni =  303/385 - loss: 0.5256 - binary_accuracy: 0.5600\n",
      "Tonni =  304/385 - loss: 0.4698 - binary_accuracy: 0.7000\n",
      "Tonni =  305/385 - loss: 0.5420 - binary_accuracy: 0.5200\n",
      "Tonni =  306/385 - loss: 0.5685 - binary_accuracy: 0.5000\n",
      "Tonni =  307/385 - loss: 0.4930 - binary_accuracy: 0.5800\n",
      "Tonni =  308/385 - loss: 0.5429 - binary_accuracy: 0.5600\n",
      "Tonni =  309/385 - loss: 0.6263 - binary_accuracy: 0.5000\n",
      "Tonni =  310/385 - loss: 0.5167 - binary_accuracy: 0.5400\n",
      "Tonni =  311/385 - loss: 0.5484 - binary_accuracy: 0.5200\n",
      "Tonni =  312/385 - loss: 0.5656 - binary_accuracy: 0.5000\n",
      "Tonni =  313/385 - loss: 0.4861 - binary_accuracy: 0.6400\n",
      "Tonni =  314/385 - loss: 0.6626 - binary_accuracy: 0.4800\n",
      "Tonni =  315/385 - loss: 0.5231 - binary_accuracy: 0.6200\n",
      "Tonni =  316/385 - loss: 0.5037 - binary_accuracy: 0.5200\n",
      "Tonni =  317/385 - loss: 0.5981 - binary_accuracy: 0.4800\n",
      "Tonni =  318/385 - loss: 0.5174 - binary_accuracy: 0.5800\n",
      "Tonni =  319/385 - loss: 0.5697 - binary_accuracy: 0.5000\n",
      "Tonni =  320/385 - loss: 0.5541 - binary_accuracy: 0.6000\n",
      "Tonni =  321/385 - loss: 0.6072 - binary_accuracy: 0.4800\n",
      "Tonni =  322/385 - loss: 0.4963 - binary_accuracy: 0.6600\n",
      "Tonni =  323/385 - loss: 0.5218 - binary_accuracy: 0.6200\n",
      "Tonni =  324/385 - loss: 0.6259 - binary_accuracy: 0.4400\n",
      "Tonni =  325/385 - loss: 0.5360 - binary_accuracy: 0.5400\n",
      "Tonni =  326/385 - loss: 0.5391 - binary_accuracy: 0.6000\n",
      "Tonni =  327/385 - loss: 0.5179 - binary_accuracy: 0.6000\n",
      "Tonni =  328/385 - loss: 0.5479 - binary_accuracy: 0.6200\n",
      "Tonni =  329/385 - loss: 0.5003 - binary_accuracy: 0.5800\n",
      "Tonni =  330/385 - loss: 0.5464 - binary_accuracy: 0.5400\n",
      "Tonni =  331/385 - loss: 0.5021 - binary_accuracy: 0.6200\n",
      "Tonni =  332/385 - loss: 0.5240 - binary_accuracy: 0.6400\n",
      "Tonni =  333/385 - loss: 0.5452 - binary_accuracy: 0.5400\n",
      "Tonni =  334/385 - loss: 0.4862 - binary_accuracy: 0.6600\n",
      "Tonni =  335/385 - loss: 0.5060 - binary_accuracy: 0.5800\n",
      "Tonni =  336/385 - loss: 0.5917 - binary_accuracy: 0.5400\n",
      "Tonni =  337/385 - loss: 0.4529 - binary_accuracy: 0.6000\n",
      "Tonni =  338/385 - loss: 0.8294 - binary_accuracy: 0.5200\n",
      "Tonni =  339/385 - loss: 0.5007 - binary_accuracy: 0.5200\n",
      "Tonni =  340/385 - loss: 0.6034 - binary_accuracy: 0.4800\n",
      "Tonni =  341/385 - loss: 0.4542 - binary_accuracy: 0.6600\n",
      "Tonni =  342/385 - loss: 0.4613 - binary_accuracy: 0.6400\n",
      "Tonni =  343/385 - loss: 0.5217 - binary_accuracy: 0.5000\n",
      "Tonni =  344/385 - loss: 0.5589 - binary_accuracy: 0.5800\n",
      "Tonni =  345/385 - loss: 0.5709 - binary_accuracy: 0.5600\n",
      "Tonni =  346/385 - loss: 0.4927 - binary_accuracy: 0.6000\n",
      "Tonni =  347/385 - loss: 0.7062 - binary_accuracy: 0.4200\n",
      "Tonni =  348/385 - loss: 0.8070 - binary_accuracy: 0.6000\n",
      "Tonni =  349/385 - loss: 0.6405 - binary_accuracy: 0.5000\n",
      "Tonni =  350/385 - loss: 0.4423 - binary_accuracy: 0.7000\n",
      "Tonni =  351/385 - loss: 0.5502 - binary_accuracy: 0.6200\n",
      "Tonni =  352/385 - loss: 0.5233 - binary_accuracy: 0.6400\n",
      "Tonni =  353/385 - loss: 0.6026 - binary_accuracy: 0.5800\n",
      "Tonni =  354/385 - loss: 0.6998 - binary_accuracy: 0.5400\n",
      "Tonni =  355/385 - loss: 0.5614 - binary_accuracy: 0.6600\n",
      "Tonni =  356/385 - loss: 0.5567 - binary_accuracy: 0.7000\n",
      "Tonni =  357/385 - loss: 0.6154 - binary_accuracy: 0.6200\n",
      "Tonni =  358/385 - loss: 0.6124 - binary_accuracy: 0.6000\n",
      "Tonni =  359/385 - loss: 0.5307 - binary_accuracy: 0.7000\n",
      "Tonni =  360/385 - loss: 0.4404 - binary_accuracy: 0.8400\n",
      "Tonni =  361/385 - loss: 0.6281 - binary_accuracy: 0.6200\n",
      "Tonni =  362/385 - loss: 0.4903 - binary_accuracy: 0.6200\n",
      "Tonni =  363/385 - loss: 0.5103 - binary_accuracy: 0.7200\n",
      "Tonni =  364/385 - loss: 0.5231 - binary_accuracy: 0.6800\n",
      "Tonni =  365/385 - loss: 0.4908 - binary_accuracy: 0.6200\n",
      "Tonni =  366/385 - loss: 0.5952 - binary_accuracy: 0.5400\n",
      "Tonni =  367/385 - loss: 0.4664 - binary_accuracy: 0.7200\n",
      "Tonni =  368/385 - loss: 0.5673 - binary_accuracy: 0.6400\n",
      "Tonni =  369/385 - loss: 0.5227 - binary_accuracy: 0.6000\n",
      "Tonni =  370/385 - loss: 0.4961 - binary_accuracy: 0.5800\n",
      "Tonni =  371/385 - loss: 0.5485 - binary_accuracy: 0.6200\n",
      "Tonni =  372/385 - loss: 0.5999 - binary_accuracy: 0.5800\n",
      "Tonni =  373/385 - loss: 0.4953 - binary_accuracy: 0.6000\n",
      "Tonni =  374/385 - loss: 0.5069 - binary_accuracy: 0.6400\n",
      "Tonni =  375/385 - loss: 0.4487 - binary_accuracy: 0.6000\n",
      "Tonni =  376/385 - loss: 0.5682 - binary_accuracy: 0.6200\n",
      "Tonni =  377/385 - loss: 0.4560 - binary_accuracy: 0.7600\n",
      "Tonni =  378/385 - loss: 0.5792 - binary_accuracy: 0.5400\n",
      "Tonni =  379/385 - loss: 0.5576 - binary_accuracy: 0.5600\n",
      "Tonni =  380/385 - loss: 0.6169 - binary_accuracy: 0.6400\n",
      "Tonni =  381/385 - loss: 0.5231 - binary_accuracy: 0.6000\n",
      "Tonni =  382/385 - loss: 0.4747 - binary_accuracy: 0.6800\n",
      "Tonni =  383/385 - loss: 0.5249 - binary_accuracy: 0.5400\n",
      "Tonni =  384/385 - loss: 0.5297 - binary_accuracy: 0.7400\n",
      "Tonni =  385/385 - loss: 0.6299 - binary_accuracy: 0.7500\n",
      "385/385 - loss: 0.6299 - binary_accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "\n",
      "Best result with 'GCN' embeddings from 'operator_hadamard'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>operator_hadamard</th>\n",
       "      <td>(0.9045504587155963, 0.9049151541296261)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_l1</th>\n",
       "      <td>(0.777394495412844, 0.819296994558928)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_l2</th>\n",
       "      <td>(0.7432660550458715, 0.7910873571853416)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator_avg</th>\n",
       "      <td>(0.6030825688073395, 0.6921808533324765)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ROC AUC\n",
       "name                                                       \n",
       "operator_hadamard  (0.9045504587155963, 0.9049151541296261)\n",
       "operator_l1          (0.777394495412844, 0.819296994558928)\n",
       "operator_l2        (0.7432660550458715, 0.7910873571853416)\n",
       "operator_avg       (0.6030825688073395, 0.6921808533324765)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gcn_result = train_and_evaluate(gcn_embedding, \"GCN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison between Node2Vec, Attri2Vec, GraphSAGE and GCN on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC AUC scores on the test set of links of different embeddings with their corresponding best operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Node2Vec</th>\n",
       "      <td>0.535055</td>\n",
       "      <td>0.560504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attri2Vec</th>\n",
       "      <td>0.673114</td>\n",
       "      <td>0.628997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GraphSAGE</th>\n",
       "      <td>0.895558</td>\n",
       "      <td>0.880547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <td>0.891337</td>\n",
       "      <td>0.876653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ROC        AP\n",
       "name                         \n",
       "Node2Vec   0.535055  0.560504\n",
       "Attri2Vec  0.673114  0.628997\n",
       "GraphSAGE  0.895558  0.880547\n",
       "GCN        0.891337  0.876653"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        (\"Node2Vec\", node2vec_result[0], node2vec_result[1]),\n",
    "        (\"Attri2Vec\", attri2vec_result[0], attri2vec_result[1]),\n",
    "        (\"GraphSAGE\", graphsage_result[0], graphsage_result[1]),\n",
    "        (\"GCN\", gcn_result[0], gcn_result[1]),\n",
    "    ],\n",
    "    columns=(\"name\", \"ROC\", \"AP\"),\n",
    ").set_index(\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example has demonstrated how to use the `stellargraph` library to build a link prediction algorithm for homogeneous graphs using the unsupervised embeddings learned by Node2Vec [1], Attri2Vec [2] and GraphSAGE [3] and GCN [4]. \n",
    "\n",
    "For more information about the link prediction process, all of these algorithms have specific demos with more details:\n",
    "\n",
    "- [Node2Vec](node2vec-link-prediction.ipynb)\n",
    "- [Attri2Vec](attri2vec-link-prediction.ipynb)\n",
    "- [GraphSAGE](graphsage-link-prediction.ipynb)\n",
    "- [GCN](gcn-link-prediction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden",
    "tags": [
     "CloudRunner"
    ]
   },
   "source": [
    "<table><tr><td>Run the latest release of this notebook:</td><td><a href=\"https://mybinder.org/v2/gh/stellargraph/stellargraph/master?urlpath=lab/tree/demos/link-prediction/homogeneous-comparison-link-prediction.ipynb\" alt=\"Open In Binder\" target=\"_parent\"><img src=\"https://mybinder.org/badge_logo.svg\"/></a></td><td><a href=\"https://colab.research.google.com/github/stellargraph/stellargraph/blob/master/demos/link-prediction/homogeneous-comparison-link-prediction.ipynb\" alt=\"Open In Colab\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a></td></tr></table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
